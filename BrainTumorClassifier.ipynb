{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUlETdhwkRln98T1JAG/it"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N4hx4FhUZHGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b76e14d-cfbc-405c-edf1-a13a05d7e616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import glob\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Link to dataset https://www.kaggle.com/datasets/thomasdubail/brain-tumors-256x256/code\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/archive (2).zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data/')\n"
      ],
      "metadata": {
        "id": "oDcrgpMrZLgl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/data/Data'\n",
        "\n",
        "# Create lists to hold image file paths and corresponding labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Define image size\n",
        "image_size = (128, 128)\n",
        "\n",
        "for folder_name in os.listdir(base_dir):\n",
        "    if os.path.isdir(os.path.join(base_dir, folder_name)):\n",
        "        file_list = glob.glob(os.path.join(base_dir, folder_name, '*.jpg'))\n",
        "        print(len(file_list))\n",
        "\n",
        "\n",
        "        for file_path in file_list:\n",
        "            image_paths.append(file_path)\n",
        "            labels.append(folder_name)\n",
        "\n",
        "# Convert the lists to numpy arrays\n",
        "image_paths = np.array(image_paths)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Load images and convert them to arrays\n",
        "images = np.array([img_to_array(load_img(img, target_size=image_size)) for img in image_paths])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esmp1mpfbNX7",
        "outputId": "f27295ee-8f86-4f5b-e87b-048ffb8b33f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "438\n",
            "913\n",
            "901\n",
            "844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Shuffle the dataset\n",
        "shuffled_indices = np.arange(images.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "images = images[shuffled_indices]\n",
        "labels_encoded = labels_encoded[shuffled_indices]\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the integer labels to one-hot vectors\n",
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_val_one_hot = to_categorical(y_val)\n",
        "\n",
        "# Check the distribution of labels to ensure there are four classes\n",
        "unique_labels, counts = np.unique(labels_encoded, return_counts=True)\n",
        "label_distribution = dict(zip(label_encoder.inverse_transform(unique_labels), counts))\n",
        "print(f\"Label distribution: {label_distribution}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx4WpxsrCFsX",
        "outputId": "af139c9a-edcf-433b-e215-886478ea7dc5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution: {'glioma_tumor': 901, 'meningioma_tumor': 913, 'normal': 438, 'pituitary_tumor': 844}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the image data generator with only rescaling\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Create the training data generator\n",
        "train_generator = train_datagen.flow(\n",
        "    X_train,\n",
        "    y_train_one_hot,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create the validation data generator\n",
        "val_generator = val_datagen.flow(\n",
        "    X_val,\n",
        "    y_val_one_hot,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "gV2hszd6b_OC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='softmax')  # Assuming 4 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(X_train) // 32,\n",
        "    epochs=30,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=len(X_val) // 32\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WDFA9dRcJTY",
        "outputId": "7c654ec9-7dac-461f-a4d3-5d95f95a0b51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "77/77 [==============================] - 7s 31ms/step - loss: 1.2723 - accuracy: 0.4173 - val_loss: 0.9383 - val_accuracy: 0.6250\n",
            "Epoch 2/30\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 0.8426 - accuracy: 0.6628 - val_loss: 0.7379 - val_accuracy: 0.7105\n",
            "Epoch 3/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.6265 - accuracy: 0.7545 - val_loss: 0.6361 - val_accuracy: 0.7747\n",
            "Epoch 4/30\n",
            "77/77 [==============================] - 2s 25ms/step - loss: 0.5008 - accuracy: 0.8081 - val_loss: 0.4848 - val_accuracy: 0.8158\n",
            "Epoch 5/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.3759 - accuracy: 0.8556 - val_loss: 0.4529 - val_accuracy: 0.8306\n",
            "Epoch 6/30\n",
            "77/77 [==============================] - 2s 25ms/step - loss: 0.2745 - accuracy: 0.8977 - val_loss: 0.4040 - val_accuracy: 0.8355\n",
            "Epoch 7/30\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 0.1900 - accuracy: 0.9272 - val_loss: 0.4314 - val_accuracy: 0.8536\n",
            "Epoch 8/30\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 0.1421 - accuracy: 0.9513 - val_loss: 0.4268 - val_accuracy: 0.8931\n",
            "Epoch 9/30\n",
            "77/77 [==============================] - 2s 28ms/step - loss: 0.1063 - accuracy: 0.9697 - val_loss: 0.4270 - val_accuracy: 0.8668\n",
            "Epoch 10/30\n",
            "77/77 [==============================] - 2s 25ms/step - loss: 0.0679 - accuracy: 0.9795 - val_loss: 0.4069 - val_accuracy: 0.8964\n",
            "Epoch 11/30\n",
            "77/77 [==============================] - 2s 24ms/step - loss: 0.0976 - accuracy: 0.9656 - val_loss: 0.4032 - val_accuracy: 0.8947\n",
            "Epoch 12/30\n",
            "77/77 [==============================] - 2s 24ms/step - loss: 0.0512 - accuracy: 0.9836 - val_loss: 0.4624 - val_accuracy: 0.8931\n",
            "Epoch 13/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.0363 - accuracy: 0.9869 - val_loss: 0.4292 - val_accuracy: 0.8947\n",
            "Epoch 14/30\n",
            "77/77 [==============================] - 2s 29ms/step - loss: 0.0397 - accuracy: 0.9881 - val_loss: 0.5296 - val_accuracy: 0.8734\n",
            "Epoch 15/30\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 0.0452 - accuracy: 0.9849 - val_loss: 0.4906 - val_accuracy: 0.8947\n",
            "Epoch 16/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.4918 - val_accuracy: 0.9046\n",
            "Epoch 17/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.0251 - accuracy: 0.9926 - val_loss: 0.5424 - val_accuracy: 0.8832\n",
            "Epoch 18/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.0335 - accuracy: 0.9918 - val_loss: 0.4988 - val_accuracy: 0.8947\n",
            "Epoch 19/30\n",
            "77/77 [==============================] - 2s 25ms/step - loss: 0.0202 - accuracy: 0.9955 - val_loss: 0.5639 - val_accuracy: 0.8882\n",
            "Epoch 20/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.0238 - accuracy: 0.9902 - val_loss: 0.5564 - val_accuracy: 0.8931\n",
            "Epoch 21/30\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 0.0485 - accuracy: 0.9832 - val_loss: 0.5956 - val_accuracy: 0.8520\n",
            "Epoch 22/30\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0489 - accuracy: 0.9836 - val_loss: 0.6452 - val_accuracy: 0.8717\n",
            "Epoch 23/30\n",
            "77/77 [==============================] - 2s 27ms/step - loss: 0.0178 - accuracy: 0.9943 - val_loss: 0.6144 - val_accuracy: 0.8980\n",
            "Epoch 24/30\n",
            "77/77 [==============================] - 2s 24ms/step - loss: 0.0097 - accuracy: 0.9980 - val_loss: 0.6192 - val_accuracy: 0.8980\n",
            "Epoch 25/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.0128 - accuracy: 0.9951 - val_loss: 0.5782 - val_accuracy: 0.8980\n",
            "Epoch 26/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.0129 - accuracy: 0.9959 - val_loss: 0.6346 - val_accuracy: 0.8997\n",
            "Epoch 27/30\n",
            "77/77 [==============================] - 2s 28ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.6424 - val_accuracy: 0.8997\n",
            "Epoch 28/30\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.6563 - val_accuracy: 0.9161\n",
            "Epoch 29/30\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 0.0291 - accuracy: 0.9906 - val_loss: 0.7205 - val_accuracy: 0.8816\n",
            "Epoch 30/30\n",
            "77/77 [==============================] - 2s 26ms/step - loss: 0.0342 - accuracy: 0.9894 - val_loss: 0.7170 - val_accuracy: 0.8717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-RJrowWFDUm"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}