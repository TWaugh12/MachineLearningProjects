{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TWaugh12/Projects/blob/main/TheTimeTravelerLanguageModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cDSMQBebIDnD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import re\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Time Traveller (for so it will be convenient to speak of him)\n",
            "was expounding a recondite matter to us. His grey eyes shone and\n",
            "twinkled, and his usually pale face was flushed and animated. The\n",
            "fire burned brightly, and the soft radiance of the incandescent\n",
            "lights in the lilies of silver caught the bubbles that flashed and\n",
            "passed in our glasses. Our chairs, being his patents, embraced and\n",
            "caressed us rather than submitted to be sat upon, and there was that\n",
            "luxurious after-dinner atmosphere when thought roams gracefully\n",
            "free of the trammels of precision. And he put it to us in this\n",
            "way--marking the points with a lean forefinger--as we sat and lazily\n"
          ]
        }
      ],
      "source": [
        "corpus = [line.strip() for line in open('./TheTimeMachine.txt') if line.strip()][2:]\n",
        "print(\"\\n\".join(corpus[:10]))\n",
        "\n",
        "# Tokenize the sentences into words. All lower case. Ignore punctuation.\n",
        "corpus = [re.sub('[^A-Za-z0-9]+', ' ', line).lower() for line in corpus]\n",
        "corpus = [re.sub(' +', ' ', line) for line in corpus]\n",
        "corpus = [word for line in corpus for word in line.split()]\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl586JYKIDnG",
        "outputId": "b67f0cdf-ac73-4d64-bcc0-cac7fab1df70"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words = 3000\n",
            "10 most popular words are: ['the', 'i', 'and', 'of', 'a', 'to', 'was', 'in', 'that', 'my']\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 2999\n",
        "tkn_counter = Counter([word for word in corpus])\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(tkn_counter.most_common(vocab_size))}\n",
        "vocab[\"/UNK\"] = len(vocab)\n",
        "print(\"Total words =\", len(vocab))\n",
        "print(\"10 most popular words are:\", list(vocab.keys())[:10])\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqLmU7nNIDnH",
        "outputId": "d6e36394-32f7-49cd-d609-e886763545a6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random snippet from the corpus.\n",
            "  * Token IDS:\t tensor([ 312,   54,   27,   42,  600,    3, 1472,  110,   15,  108,  439,    3,\n",
            "          18,  108,   72,  130,    4,  849,   51,   52,  370,  187,    3, 1472,\n",
            "        2275,  231,  182,    0,  235,   17,    4, 1473,   64,   37,  371,  151,\n",
            "         130,    0,  849,    7,   20, 2276,   26,  188,  219,   63,  140, 1462,\n",
            "           7,    4])\n",
            "  * Words:\t\t course we have no means of staying back for any length of time any more than a savage or an animal has of staying six feet above the ground but a civilized man is better off than the savage in this respect he can go up against gravitation in a\n"
          ]
        }
      ],
      "source": [
        "class TextCorpusDataset(Dataset):\n",
        "    def __init__(self, corpus, vocab, snippet_len=50):\n",
        "        super().__init__()\n",
        "        self.corpus = corpus\n",
        "        self.snippet_len = snippet_len\n",
        "\n",
        "        # Vocabulary (word-to-index mapping)\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # Inverse vocabulary (index-to-word mapping)\n",
        "        self.inv_vocab = {idx: word for word, idx in self.vocab.items()}\n",
        "\n",
        "    def convert2idx(self, word_sequence):\n",
        "        return [self.vocab[word if word in self.vocab else \"/UNK\"] for word in word_sequence]\n",
        "\n",
        "    def convert2words(self, idx_sequence):\n",
        "        return [self.inv_vocab[idx] for idx in idx_sequence]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.corpus) - self.snippet_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        snippet = self.corpus[idx:idx+self.snippet_len]\n",
        "        snippet = torch.tensor(self.convert2idx(snippet))\n",
        "        return snippet\n",
        "\n",
        "# Test dataset function\n",
        "dataset = TextCorpusDataset(corpus, vocab, snippet_len=50)\n",
        "snippet = dataset[1234]\n",
        "print(\"\\nRandom snippet from the corpus.\")\n",
        "print(\"  * Token IDS:\\t\", snippet)\n",
        "print(\"  * Words:\\t\\t\", \" \".join([dataset.inv_vocab[i] for i in snippet.tolist()]))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26RF0nfMIDnH",
        "outputId": "c4bc7c76-955c-4aa8-b677-c5a97e269b97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "349QnKRGIDnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46778c1-bd37-43ab-b94c-13ffbf586fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 6.0542\n",
            "Epoch [2/100], Loss: 4.6372\n",
            "Epoch [3/100], Loss: 3.8251\n",
            "Epoch [4/100], Loss: 3.1766\n",
            "Epoch [5/100], Loss: 2.6902\n",
            "Epoch [6/100], Loss: 2.3215\n",
            "Epoch [7/100], Loss: 2.0395\n",
            "Epoch [8/100], Loss: 1.8193\n",
            "Epoch [9/100], Loss: 1.6479\n",
            "Epoch [10/100], Loss: 1.5152\n",
            "Epoch [11/100], Loss: 1.4047\n",
            "Epoch [12/100], Loss: 1.3189\n",
            "Epoch [13/100], Loss: 1.2475\n",
            "Epoch [14/100], Loss: 1.1886\n",
            "Epoch [15/100], Loss: 1.1378\n",
            "Epoch [16/100], Loss: 1.0949\n",
            "Epoch [17/100], Loss: 1.0573\n",
            "Epoch [18/100], Loss: 1.0262\n",
            "Epoch [19/100], Loss: 1.0001\n",
            "Epoch [20/100], Loss: 0.9727\n",
            "Epoch [21/100], Loss: 0.9518\n",
            "Epoch [22/100], Loss: 0.9344\n",
            "Epoch [23/100], Loss: 0.9156\n",
            "Epoch [24/100], Loss: 0.8993\n",
            "Epoch [25/100], Loss: 0.8884\n",
            "Epoch [26/100], Loss: 0.8741\n",
            "Epoch [27/100], Loss: 0.8583\n",
            "Epoch [28/100], Loss: 0.8531\n",
            "Epoch [29/100], Loss: 0.8420\n",
            "Epoch [30/100], Loss: 0.8327\n",
            "Epoch [31/100], Loss: 0.8223\n",
            "Epoch [32/100], Loss: 0.8160\n",
            "Epoch [33/100], Loss: 0.8083\n",
            "Epoch [34/100], Loss: 0.8014\n",
            "Epoch [35/100], Loss: 0.7997\n",
            "Epoch [36/100], Loss: 0.7913\n",
            "Epoch [37/100], Loss: 0.7866\n",
            "Epoch [38/100], Loss: 0.7781\n",
            "Epoch [39/100], Loss: 0.7723\n",
            "Epoch [40/100], Loss: 0.7679\n",
            "Epoch [41/100], Loss: 0.7680\n",
            "Epoch [42/100], Loss: 0.7646\n",
            "Epoch [43/100], Loss: 0.7581\n",
            "Epoch [44/100], Loss: 0.7560\n",
            "Epoch [45/100], Loss: 0.7512\n",
            "Epoch [46/100], Loss: 0.7454\n",
            "Epoch [47/100], Loss: 0.7449\n",
            "Epoch [48/100], Loss: 0.7416\n",
            "Epoch [49/100], Loss: 0.7377\n",
            "Epoch [50/100], Loss: 0.7389\n",
            "Epoch [51/100], Loss: 0.7340\n",
            "Epoch [52/100], Loss: 0.7310\n",
            "Epoch [53/100], Loss: 0.7291\n",
            "Epoch [54/100], Loss: 0.7262\n",
            "Epoch [55/100], Loss: 0.7252\n",
            "Epoch [56/100], Loss: 0.7221\n",
            "Epoch [57/100], Loss: 0.7212\n",
            "Epoch [58/100], Loss: 0.7182\n",
            "Epoch [59/100], Loss: 0.7154\n",
            "Epoch [60/100], Loss: 0.7129\n",
            "Epoch [61/100], Loss: 0.7118\n",
            "Epoch [62/100], Loss: 0.7105\n",
            "Epoch [63/100], Loss: 0.7096\n",
            "Epoch [64/100], Loss: 0.7080\n",
            "Epoch [65/100], Loss: 0.7060\n",
            "Epoch [66/100], Loss: 0.7040\n",
            "Epoch [67/100], Loss: 0.7055\n",
            "Epoch [68/100], Loss: 0.7017\n",
            "Epoch [69/100], Loss: 0.6985\n",
            "Epoch [70/100], Loss: 0.7022\n",
            "Epoch [71/100], Loss: 0.6979\n",
            "Epoch [72/100], Loss: 0.6987\n",
            "Epoch [73/100], Loss: 0.6957\n",
            "Epoch [74/100], Loss: 0.6937\n",
            "Epoch [75/100], Loss: 0.6928\n",
            "Epoch [76/100], Loss: 0.6903\n",
            "Epoch [77/100], Loss: 0.6925\n",
            "Epoch [78/100], Loss: 0.6910\n",
            "Epoch [79/100], Loss: 0.6899\n",
            "Epoch [80/100], Loss: 0.6860\n",
            "Epoch [81/100], Loss: 0.6874\n",
            "Epoch [82/100], Loss: 0.6854\n",
            "Epoch [83/100], Loss: 0.6858\n",
            "Epoch [84/100], Loss: 0.6855\n",
            "Epoch [85/100], Loss: 0.6849\n",
            "Epoch [86/100], Loss: 0.6823\n",
            "Epoch [87/100], Loss: 0.6829\n",
            "Epoch [88/100], Loss: 0.6787\n",
            "Epoch [89/100], Loss: 0.6784\n",
            "Epoch [90/100], Loss: 0.6797\n",
            "Epoch [91/100], Loss: 0.6807\n",
            "Epoch [92/100], Loss: 0.6775\n",
            "Epoch [93/100], Loss: 0.6770\n",
            "Epoch [94/100], Loss: 0.6762\n",
            "Epoch [95/100], Loss: 0.6762\n",
            "Epoch [96/100], Loss: 0.6750\n",
            "Epoch [97/100], Loss: 0.6730\n",
            "Epoch [98/100], Loss: 0.6747\n",
            "Epoch [99/100], Loss: 0.6745\n",
            "Epoch [100/100], Loss: 0.6723\n"
          ]
        }
      ],
      "source": [
        "# Define the Word2Vec CBOW model\n",
        "class Word2Vec_CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        context_embeds = self.embeddings(context)\n",
        "        avg_context_embeds = context_embeds.mean(dim=1)\n",
        "        logits = self.linear(avg_context_embeds)\n",
        "        return logits\n",
        "\n",
        "# Hyperparameters\n",
        "context_len = 2\n",
        "vocab_size = len(dataset.vocab)\n",
        "embedding_dim = 128\n",
        "learning_rate = 5e-3\n",
        "num_epochs = 100\n",
        "\n",
        "# Create DataLoader for batch training\n",
        "dataset = TextCorpusDataset(corpus, vocab, snippet_len=2*context_len + 1)\n",
        "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Create and train the CBOW model\n",
        "w2v = Word2Vec_CBOW(vocab_size, embedding_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(w2v.parameters(), lr=learning_rate)\n",
        "context_idx = [idx for idx in range(2*context_len+1) if idx != context_len]\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for snippet in train_loader:\n",
        "        context = snippet[:, context_idx].to(device)\n",
        "        target = snippet[:, context_len].to(device)\n",
        "        logits = w2v(context)\n",
        "        loss = criterion(logits, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() / len(train_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate correclty predicted words\n",
        "def calculate_accuracy(model, data_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for snippet in data_loader:\n",
        "            context = snippet[:, context_idx].to(device)\n",
        "            target = snippet[:, context_len].to(device)\n",
        "            logits = model(context)\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# Calculate accuracy on the training set\n",
        "accuracy = calculate_accuracy(w2v, train_loader)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTOKNrUvVgUU",
        "outputId": "6298fb36-6f03-4009-9d6f-88c6146bb33e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 84.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_missing_word(model, sentence, vocab, context_len):\n",
        "    model.eval()  # set the model to evaluation mode\n",
        "    words = sentence.split()\n",
        "    for i in range(context_len, len(words) - context_len):\n",
        "        context = words[i-context_len:i] + words[i+1:i+1+context_len]\n",
        "        context_indices = [vocab[word] for word in context if word in vocab]\n",
        "        context_tensor = torch.tensor(context_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(context_tensor)\n",
        "        predicted_index = torch.argmax(logits, dim=1).item()\n",
        "        predicted_word = [word for word, idx in vocab.items() if idx == predicted_index][0]\n",
        "        print(f\"Context: {' '.join(context)} -> Predicted word: {predicted_word}\")"
      ],
      "metadata": {
        "id": "ro6yqfV1V4Mg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "# Extract the word embeddings to analyze it\n",
        "word_embeddings = w2v.embeddings.weight.detach().cpu().numpy()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "jVd8aVZIIDnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Normalize the embeddings\n",
        "normalized_embeddings = word_embeddings / np.linalg.norm(word_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Function to find nearest neighbors\n",
        "def find_nearest_neighbors(word, embeddings, vocab, num_neighbors=5):\n",
        "    if word not in vocab:\n",
        "        return f\"{word} not in vocabulary.\"\n",
        "\n",
        "    word_index = vocab[word]\n",
        "    word_embedding = embeddings[word_index].reshape(1, -1)\n",
        "\n",
        "    # Compute similarities\n",
        "    similarities = cosine_similarity(word_embedding, embeddings)[0]\n",
        "\n",
        "    # Get the indices of the most similar embeddings\n",
        "    most_similar_indices = np.argsort(-similarities)[1:num_neighbors+1]\n",
        "\n",
        "    # Map indices back to words\n",
        "    index_to_word = {idx: word for word, idx in vocab.items()}\n",
        "    nearest_neighbors = [index_to_word[idx] for idx in most_similar_indices]\n",
        "\n",
        "    return nearest_neighbors\n",
        "\n",
        "# Example usage\n",
        "word = 'actually'\n",
        "nearest_neighbors = find_nearest_neighbors(word, normalized_embeddings, dataset.vocab)\n",
        "print(f\"Nearest neighbors of '{word}': {nearest_neighbors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5pgLVwGXU__",
        "outputId": "ba13149e-4241-4d34-b2eb-ff7628204ec9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbors of 'actually': ['distance', 'rushed', 'curtain', 'lucid', 'frankness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": [
        "class NextWordPredictionMLP(nn.Module):\n",
        "    def __init__(self, num_context, embedding, depth=3, hidden_dim=50):\n",
        "        super().__init__()\n",
        "        self.embedding = embedding\n",
        "\n",
        "        self.mlp = nn.Sequential()\n",
        "        for d in range(depth):\n",
        "            if d == 0:\n",
        "                in_chans = num_context * embedding.embedding_dim\n",
        "                out_chans = hidden_dim\n",
        "            elif d == depth - 1:\n",
        "                in_chans = hidden_dim\n",
        "                out_chans = embedding.num_embeddings\n",
        "            else:\n",
        "                in_chans = out_chans = hidden_dim\n",
        "\n",
        "            self.mlp.add_module(f'linear{d}', nn.Linear(in_chans, out_chans))\n",
        "            self.mlp.add_module(f'bn{d}', nn.BatchNorm1d(out_chans))\n",
        "            self.mlp.add_module(f'act{d}', nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, context):\n",
        "        emb = self.embedding(context).flatten(1)\n",
        "        return self.mlp(emb)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WksWx_60IDnI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ep000] | Loss 7.636 \t Perplexity  2071.029\n",
            "[Ep001] | Loss 6.559 \t Perplexity  705.237\n",
            "[Ep002] | Loss 5.897 \t Perplexity  364.012\n",
            "[Ep003] | Loss 5.414 \t Perplexity  224.534\n",
            "[Ep004] | Loss 5.062 \t Perplexity  157.873\n",
            "[Ep005] | Loss 4.781 \t Perplexity  119.250\n",
            "[Ep006] | Loss 4.563 \t Perplexity  95.882\n",
            "[Ep007] | Loss 4.399 \t Perplexity  81.373\n",
            "[Ep008] | Loss 4.254 \t Perplexity  70.400\n",
            "[Ep009] | Loss 4.125 \t Perplexity  61.898\n",
            "[Ep010] | Loss 4.016 \t Perplexity  55.467\n",
            "[Ep011] | Loss 3.920 \t Perplexity  50.392\n",
            "[Ep012] | Loss 3.828 \t Perplexity  45.972\n",
            "[Ep013] | Loss 3.755 \t Perplexity  42.721\n",
            "[Ep014] | Loss 3.685 \t Perplexity  39.851\n",
            "[Ep015] | Loss 3.615 \t Perplexity  37.160\n",
            "[Ep016] | Loss 3.544 \t Perplexity  34.591\n",
            "[Ep017] | Loss 3.485 \t Perplexity  32.615\n",
            "[Ep018] | Loss 3.437 \t Perplexity  31.101\n",
            "[Ep019] | Loss 3.373 \t Perplexity  29.174\n",
            "[Ep020] | Loss 3.328 \t Perplexity  27.877\n",
            "[Ep021] | Loss 3.276 \t Perplexity  26.472\n",
            "[Ep022] | Loss 3.230 \t Perplexity  25.284\n",
            "[Ep023] | Loss 3.186 \t Perplexity  24.182\n",
            "[Ep024] | Loss 3.144 \t Perplexity  23.193\n",
            "[Ep025] | Loss 3.103 \t Perplexity  22.274\n",
            "[Ep026] | Loss 3.064 \t Perplexity  21.419\n",
            "[Ep027] | Loss 3.015 \t Perplexity  20.390\n",
            "[Ep028] | Loss 2.979 \t Perplexity  19.674\n",
            "[Ep029] | Loss 2.938 \t Perplexity  18.884\n",
            "[Ep030] | Loss 2.902 \t Perplexity  18.213\n",
            "[Ep031] | Loss 2.867 \t Perplexity  17.579\n",
            "[Ep032] | Loss 2.825 \t Perplexity  16.854\n",
            "[Ep033] | Loss 2.798 \t Perplexity  16.414\n",
            "[Ep034] | Loss 2.762 \t Perplexity  15.827\n",
            "[Ep035] | Loss 2.739 \t Perplexity  15.474\n",
            "[Ep036] | Loss 2.707 \t Perplexity  14.980\n",
            "[Ep037] | Loss 2.669 \t Perplexity  14.428\n",
            "[Ep038] | Loss 2.645 \t Perplexity  14.080\n",
            "[Ep039] | Loss 2.608 \t Perplexity  13.572\n",
            "[Ep040] | Loss 2.579 \t Perplexity  13.185\n",
            "[Ep041] | Loss 2.551 \t Perplexity  12.819\n",
            "[Ep042] | Loss 2.524 \t Perplexity  12.473\n",
            "[Ep043] | Loss 2.486 \t Perplexity  12.016\n",
            "[Ep044] | Loss 2.464 \t Perplexity  11.747\n",
            "[Ep045] | Loss 2.435 \t Perplexity  11.420\n",
            "[Ep046] | Loss 2.414 \t Perplexity  11.176\n",
            "[Ep047] | Loss 2.396 \t Perplexity  10.974\n",
            "[Ep048] | Loss 2.358 \t Perplexity  10.575\n",
            "[Ep049] | Loss 2.334 \t Perplexity  10.317\n",
            "[Ep050] | Loss 2.318 \t Perplexity  10.154\n",
            "[Ep051] | Loss 2.286 \t Perplexity  9.838\n",
            "[Ep052] | Loss 2.263 \t Perplexity  9.615\n",
            "[Ep053] | Loss 2.239 \t Perplexity  9.387\n",
            "[Ep054] | Loss 2.216 \t Perplexity  9.169\n",
            "[Ep055] | Loss 2.195 \t Perplexity  8.979\n",
            "[Ep056] | Loss 2.175 \t Perplexity  8.802\n",
            "[Ep057] | Loss 2.159 \t Perplexity  8.661\n",
            "[Ep058] | Loss 2.131 \t Perplexity  8.427\n",
            "[Ep059] | Loss 2.107 \t Perplexity  8.223\n",
            "[Ep060] | Loss 2.085 \t Perplexity  8.046\n",
            "[Ep061] | Loss 2.065 \t Perplexity  7.889\n",
            "[Ep062] | Loss 2.050 \t Perplexity  7.770\n",
            "[Ep063] | Loss 2.034 \t Perplexity  7.641\n",
            "[Ep064] | Loss 2.011 \t Perplexity  7.474\n",
            "[Ep065] | Loss 1.993 \t Perplexity  7.336\n",
            "[Ep066] | Loss 1.969 \t Perplexity  7.166\n",
            "[Ep067] | Loss 1.954 \t Perplexity  7.054\n",
            "[Ep068] | Loss 1.940 \t Perplexity  6.957\n",
            "[Ep069] | Loss 1.915 \t Perplexity  6.788\n",
            "[Ep070] | Loss 1.893 \t Perplexity  6.641\n",
            "[Ep071] | Loss 1.873 \t Perplexity  6.507\n",
            "[Ep072] | Loss 1.870 \t Perplexity  6.488\n",
            "[Ep073] | Loss 1.842 \t Perplexity  6.312\n",
            "[Ep074] | Loss 1.836 \t Perplexity  6.270\n",
            "[Ep075] | Loss 1.808 \t Perplexity  6.096\n",
            "[Ep076] | Loss 1.793 \t Perplexity  6.010\n",
            "[Ep077] | Loss 1.778 \t Perplexity  5.915\n",
            "[Ep078] | Loss 1.760 \t Perplexity  5.813\n",
            "[Ep079] | Loss 1.743 \t Perplexity  5.716\n",
            "[Ep080] | Loss 1.728 \t Perplexity  5.629\n",
            "[Ep081] | Loss 1.719 \t Perplexity  5.576\n",
            "[Ep082] | Loss 1.696 \t Perplexity  5.453\n",
            "[Ep083] | Loss 1.683 \t Perplexity  5.383\n",
            "[Ep084] | Loss 1.671 \t Perplexity  5.319\n",
            "[Ep085] | Loss 1.651 \t Perplexity  5.213\n",
            "[Ep086] | Loss 1.640 \t Perplexity  5.156\n",
            "[Ep087] | Loss 1.622 \t Perplexity  5.065\n",
            "[Ep088] | Loss 1.602 \t Perplexity  4.965\n",
            "[Ep089] | Loss 1.598 \t Perplexity  4.941\n",
            "[Ep090] | Loss 1.585 \t Perplexity  4.879\n",
            "[Ep091] | Loss 1.566 \t Perplexity  4.786\n",
            "[Ep092] | Loss 1.551 \t Perplexity  4.718\n",
            "[Ep093] | Loss 1.540 \t Perplexity  4.666\n",
            "[Ep094] | Loss 1.525 \t Perplexity  4.597\n",
            "[Ep095] | Loss 1.507 \t Perplexity  4.514\n",
            "[Ep096] | Loss 1.501 \t Perplexity  4.484\n",
            "[Ep097] | Loss 1.485 \t Perplexity  4.414\n",
            "[Ep098] | Loss 1.473 \t Perplexity  4.360\n",
            "[Ep099] | Loss 1.462 \t Perplexity  4.312\n"
          ]
        }
      ],
      "source": [
        "def train_one_epoch(model, loss_fcn, optimizer, dataloader):\n",
        "    total_loss = 0.\n",
        "    for it, batch in enumerate(dataloader):\n",
        "        batch_past = batch[:, :T].to(device)\n",
        "        batch_now = batch[:, -1].to(device)\n",
        "\n",
        "        pred_now = model(batch_past)\n",
        "        l = loss_fcn(pred_now, batch_now)\n",
        "        total_loss += l.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    total_loss = total_loss / len(dataloader)\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def fit(model, loss_fcn, dataloader, optimizer, epochs=30):\n",
        "    for ep in range(epochs):\n",
        "        loss = train_one_epoch(model, loss_fcn, optimizer, dataloader)\n",
        "        print(f\"[Ep{ep:03}] | Loss {loss:.3f} \\t Perplexity  {np.exp(loss):.3f}\")\n",
        "\n",
        "\n",
        "T = 10\n",
        "dataset = TextCorpusDataset(corpus, vocab, snippet_len=T+1)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "\n",
        "model = NextWordPredictionMLP(T, w2v.embeddings, depth=2, hidden_dim=50).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "loss_fcn = F.cross_entropy\n",
        "\n",
        "fit(model, loss_fcn, dataloader, opt, epochs=100)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0MD9oEwIDnJ",
        "outputId": "66ecd45b-093d-4a4d-bee2-0942ebf4ae2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT: the time traveller for so it will be convenient to\n",
            "speak of him was not huge fear for that what was change of looked round me i was heard a slight s further and little people there in the shadow for a sudden i gave the two view the fire to make his hand and i felt where is very large as was that the world had always weena proceeded them from weena but really of weena had matches where day i looked under them the long for beautiful day been intense red place to full and the little in a most animals that was time in the thing was "
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    prompt = \" \".join(corpus[:10])\n",
        "    print(\"PROMPT:\", prompt)\n",
        "    context = torch.tensor([dataset.vocab[word] for word in prompt.split()]).to(device)\n",
        "    context = context.unsqueeze(0)  # Reshape it into a batch of 1\n",
        "    model.train(False)\n",
        "    for _ in range(100):\n",
        "        next_word_logits = model(context)\n",
        "        next_word_idx = next_word_logits[:, :-1].argmax(dim=1)\n",
        "        next_word = dataset.inv_vocab[next_word_idx[0].item()]\n",
        "        context = torch.cat((context[:, 1:], next_word_idx.unsqueeze(1)), 1)\n",
        "        print(next_word, end=' ')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "Zqzxw13nIDnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639efa91-0470-4f04-d80f-46846fd05ca1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "xOnrU5VLIDnJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "afTc7-b3IDnJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "SO4xSnhjIDnJ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}